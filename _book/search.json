[{"path":"index.html","id":"sobre-esta-página","chapter":"Capítulo 1 Sobre esta página","heading":"Capítulo 1 Sobre esta página","text":"La idea principal es jugar con bookdown y mostrar los resultados obtenidos en la lista de ejercicios de la materia. La mayoría de ejercicios, provienen del libro Introudction Statistical Learning 2nd Ed.:islr","code":""},{"path":"statistical-learning.html","id":"statistical-learning","chapter":"Capítulo 2 Statistical learning","heading":"Capítulo 2 Statistical learning","text":"","code":""},{"path":"statistical-learning.html","id":"ejercicio-extra","chapter":"Capítulo 2 Statistical learning","heading":"Ejercicio extra","text":"Pasar en limpioAlt","code":""},{"path":"statistical-learning.html","id":"ejercicio-1","chapter":"Capítulo 2 Statistical learning","heading":"Ejercicio 1","text":"parts () (d), indicate whether generally expect performance flexible statistical learning method better worse inflexible method. Justify answer.() sample size \\(n\\) extremely large, number predictors \\(p\\) small.\nAl que tenemos un tamaño de muestra lo suficientemente grande y estamos restrictos una cantidad chica de predictores, es útil utilizar un método flexible ya que tenemos la posibilidad de realizar ‘aproximaciones locales’ sin\narriesgarnos al problema del sobreajuste.\nEl tener un tamaño de muestra grande implica que pequeñas perturbaciones en nuestros datos la flexibilidad que demos al modelo, afecten en el ajuste de los parámetros.() sample size \\(n\\) extremely large, number predictors \\(p\\) small.Al que tenemos un tamaño de muestra lo suficientemente grande y estamos restrictos una cantidad chica de predictores, es útil utilizar un método flexible ya que tenemos la posibilidad de realizar ‘aproximaciones locales’ sin\narriesgarnos al problema del sobreajuste.El tener un tamaño de muestra grande implica que pequeñas perturbaciones en nuestros datos la flexibilidad que demos al modelo, afecten en el ajuste de los parámetros.(b)number predictors \\(p\\) extremely large, number observations \\(n\\) small.\nEste caso es el contrario al anterior, al tener una cantidad grande de predictores y una muestra chica, al usar un método flexible corremos un alto riesgo de estar frente un modelo que sobreajusta y con poco poder predictivo fuera\nde la muestra.\nAdemás, si consideramos el costo computacional de tener una gran cantidad de parámetros utilizar modelos flexibles el costo es aún mayor que uno menos flexible. Como mencioné anteriormente, al tener un tamaño de muestra pequeño,\npequeñas perturbaciones en los datos puede llegar diferentes conclusiones si utilizamos modelos flexible.(b)number predictors \\(p\\) extremely large, number observations \\(n\\) small.Este caso es el contrario al anterior, al tener una cantidad grande de predictores y una muestra chica, al usar un método flexible corremos un alto riesgo de estar frente un modelo que sobreajusta y con poco poder predictivo fuera\nde la muestra.Además, si consideramos el costo computacional de tener una gran cantidad de parámetros utilizar modelos flexibles el costo es aún mayor que uno menos flexible. Como mencioné anteriormente, al tener un tamaño de muestra pequeño,\npequeñas perturbaciones en los datos puede llegar diferentes conclusiones si utilizamos modelos flexible.(c) relationship predictors response highly non-linear.\nPara esta situación es altamente recomendable utilizar un método flexible para tratar de modelar esa linealidad en la que nos enfrentamos inicialmente, un ejemplo los modelos GAMM.(c) relationship predictors response highly non-linear.Para esta situación es altamente recomendable utilizar un método flexible para tratar de modelar esa linealidad en la que nos enfrentamos inicialmente, un ejemplo los modelos GAMM.(d) variance error terms, .e. \\(\\sigma^{2} = Var(\\varepsilon)\\), extremely high.\nDentro del ECM podemos descomponer entre un error reducible y uno irreducible, por hipotesís este último es alto, el error que se encuentra bajo nuestro controlo refiere la discrepencia entre nuestra variable de respuesta\ny nuestro modelo, la elección de un método inflexible sería una buena idea ya que en este caso reducimos las chances de obtener sobreajuste debido utilizar un modelo demasiado flexible.(d) variance error terms, .e. \\(\\sigma^{2} = Var(\\varepsilon)\\), extremely high.Dentro del ECM podemos descomponer entre un error reducible y uno irreducible, por hipotesís este último es alto, el error que se encuentra bajo nuestro controlo refiere la discrepencia entre nuestra variable de respuesta\ny nuestro modelo, la elección de un método inflexible sería una buena idea ya que en este caso reducimos las chances de obtener sobreajuste debido utilizar un modelo demasiado flexible.","code":""},{"path":"statistical-learning.html","id":"ejercicio-5","chapter":"Capítulo 2 Statistical learning","heading":"Ejercicio 5","text":"advantages disadvantages flexible (versus less flexible) approach regression classification? circumstances might flexible approach preferred less flexible approach? might less flexible approach preferred?Utilizar un entoque muy flexible nos permite realizar un mayor ajuste que respecto un modelo lineal, reducimos el sesgo pero aumentamos la varianza. Desde el punto de vista computacional debemos de estimar una mayor cantidad de parámetros,\nen algunas situaciones puede ser un tema considerar en el caso que nuestro problema sea ‘usual’, esto decir, implica recurrir métodos numéricos que quizás esten implementados, sean demasiado costosos o el único camino sea utilizar simulaciones para aproximar la solución.En algunos casos se recomienda utilizar modelos inflexibles tanto en el contexto de regressión o clasificación cuando tenemos un tamaño de muestra reducido ya que en caso contrario modelos flexibles sufren del problema del sobreajuste,\ndificultando la inferencia además de perder interpretabilidad en algun tipo de especificación.","code":""},{"path":"statistical-learning.html","id":"ejercicio-6","chapter":"Capítulo 2 Statistical learning","heading":"Ejercicio 6","text":"Describe differences parametric non-parametric statistical learning approach. advantages parametric approach regression classification (opposed nonparametric approach)? disadvantages?Utilizar un enfoque parámetrico para el aprendizaje estadístico puede resultar mas sencillo ya que nuestro problema (veces tan trivial) se reduce estimar los \\(p\\) regresores aunque siempre estamos sujetos la elección de la parametrización del modelo,\nes decir, siempre tenemos el sesgo de la elección del modelo correcto, en cambio al utilizar un enfoque parámetrico al asumir una forma en concreto para el modelo nos enfrentamos esto pero debemos de tener una muestra\nlo suficientemente grande para llegar conclusiones razonables.","code":""},{"path":"linear-regression.html","id":"linear-regression","chapter":"Capítulo 3 Linear regression","heading":"Capítulo 3 Linear regression","text":"","code":""},{"path":"linear-regression.html","id":"ejercicio-4","chapter":"Capítulo 3 Linear regression","heading":"Ejercicio 4","text":"collect set data (n = 100 observations) containing single predictor quantitative response. fit linear regression model data, well separate cubic regression, .e. \\(Y = \\beta_{0} + \\beta_{1}X + \\beta_{2}X^{2} + \\beta_{3}X^{3} + \\varepsilon\\).() Suppose true relationship X Y linear, .e. \\(Y = \\beta_{0} + \\beta_{1}X + \\varepsilon\\). Consider training residual sum squares (\\(RSS\\)) linear regression, also training \\(RSS\\) cubic regression. expect one lower , expect , enough information tell? Justify answer.\nAl que estamos bajo el supuesto que la relación entre la variable respuesta es exactamente lineal y que el la esperanza del RSS lo podemos descomponer en :\n\\[\\begin{equation*}\n      E(RSS) = E\\left[\\left(Y - \\hat{Y}\\right)^{2}\\right] = E\\left[f(X) + \\varepsilon - \\hat{f}(X)\\right]^{2} = \\underbrace{\\left[f(X) - \\hat{f}(X)\\right]^{2}}_{\\longrightarrow 0} + Var(\\varepsilon)\n  \\end{equation*}\\]\nEl primer sumando tiende cero, ya que la relación es exactamente lineal y en el caso que nuestra \\(\\hat{f}(X)\\) considere una forma cúbica el sesgo por la especificación sería mayor que en este caso.() Suppose true relationship X Y linear, .e. \\(Y = \\beta_{0} + \\beta_{1}X + \\varepsilon\\). Consider training residual sum squares (\\(RSS\\)) linear regression, also training \\(RSS\\) cubic regression. expect one lower , expect , enough information tell? Justify answer.Al que estamos bajo el supuesto que la relación entre la variable respuesta es exactamente lineal y que el la esperanza del RSS lo podemos descomponer en :\\[\\begin{equation*}\n      E(RSS) = E\\left[\\left(Y - \\hat{Y}\\right)^{2}\\right] = E\\left[f(X) + \\varepsilon - \\hat{f}(X)\\right]^{2} = \\underbrace{\\left[f(X) - \\hat{f}(X)\\right]^{2}}_{\\longrightarrow 0} + Var(\\varepsilon)\n  \\end{equation*}\\]El primer sumando tiende cero, ya que la relación es exactamente lineal y en el caso que nuestra \\(\\hat{f}(X)\\) considere una forma cúbica el sesgo por la especificación sería mayor que en este caso.(b) Answer () using test rather training \\(RSS\\).\nComo se comento anteriormente, se esperaría que el \\(RSS\\) de training del modelo de regresión lineal sea mas bajo que el del modelo cúbico, debido que la verdadera relación es lineal.\nEl modelo cúbico al intentar ajustar con una especificación incorrecta es mas sensible cambios en la muestra, por lo tanto esperaría una mayor varianza en el término de error.(b) Answer () using test rather training \\(RSS\\).Como se comento anteriormente, se esperaría que el \\(RSS\\) de training del modelo de regresión lineal sea mas bajo que el del modelo cúbico, debido que la verdadera relación es lineal.\nEl modelo cúbico al intentar ajustar con una especificación incorrecta es mas sensible cambios en la muestra, por lo tanto esperaría una mayor varianza en el término de error.(c) Suppose true relationship X Y linear, don’t know far linear. Consider training \\(RSS\\) linear regression, also training \\(RSS\\) cubic regression. expect one lower , expect , enough information tell? Justify answer.\nEn este caso, el modelo cúbico tendría un menor \\(RSS\\) de training debido que es un modelo mas flexible ya que seguira mas de cerca los puntos.(c) Suppose true relationship X Y linear, don’t know far linear. Consider training \\(RSS\\) linear regression, also training \\(RSS\\) cubic regression. expect one lower , expect , enough information tell? Justify answer.En este caso, el modelo cúbico tendría un menor \\(RSS\\) de training debido que es un modelo mas flexible ya que seguira mas de cerca los puntos.(d) Answer (c) using test rather training \\(RSS\\).\nSi bien sabemos que la relación es lineal, sabemos que órden tiene la función polinomial que generan estos datos, modelos lineales tendrían un menor sesgo por su flexibilidad aunque este puede crecer de forma considerable\nen el término de la varianza del ruido si el órden es el correcto por más que sea un error sumamente saturado.(d) Answer (c) using test rather training \\(RSS\\).Si bien sabemos que la relación es lineal, sabemos que órden tiene la función polinomial que generan estos datos, modelos lineales tendrían un menor sesgo por su flexibilidad aunque este puede crecer de forma considerable\nen el término de la varianza del ruido si el órden es el correcto por más que sea un error sumamente saturado.","code":""},{"path":"moving-beyond-linearity.html","id":"moving-beyond-linearity","chapter":"Capítulo 4 Moving Beyond Linearity","heading":"Capítulo 4 Moving Beyond Linearity","text":"","code":""},{"path":"moving-beyond-linearity.html","id":"ejercicio-1-1","chapter":"Capítulo 4 Moving Beyond Linearity","heading":"Ejercicio 1","text":"mentioned chapter cubic regression spline one knot \\(\\xi\\) can obtained using basis form \\(x\\), \\(x^{2}\\), \\(x^{3}\\), \\((x - \\xi{})^{3}_{+}\\) ,\\((x − \\xi{})^{3}_{+} = (x − \\xi)^{3}\\) \\(~ x> \\xi\\) equals 0 otherwise. now show function form\\[\\begin{equation*}\n    f(x) = \\beta_{0} + \\beta_{1}x + \\beta_{2}x^{2} + \\beta_{3}x^{3} + \\beta_{4}(x-\\xi{})^{3}_{+}\n\\end{equation*}\\]indeed cubic regression spline, regardless values \\(\\beta_{0}\\), \\(\\beta_{1}\\), \\(\\beta_{2}\\), \\(\\beta_{3}\\), \\(\\beta_{4}\\).() Find cubic polynomial\n\\[\\begin{equation*}\n      f_{1}(x) = a_{1} + b_{1}x + c_{1}x^{2} + d_{1}x^{3}\n  \\end{equation*}\\]\nf(x)= f1(x) \\(x \\leq \\xi{}\\). Express \\(a_{1}\\),\\(b_{1}\\),\\(c_{1}\\),\\(d_{1}\\) terms \\(\\beta_{0}\\),\\(\\beta_{1}\\), \\(\\beta_{2}\\), \\(\\beta_{3}\\), \\(\\beta_{4}\\)\nEn este caso, al que la función \\((x - \\xi{})^{3}_{+}\\), cada coeficiente corresponde su análogo ordenados de mayor menor, es decir \\(a_{1} = \\beta_{0}\\), \\(b_{1} = \\beta_{1}\\), \\(c_{1} = \\beta_{2}\\), \\(d_{1} = \\beta_{3}\\)() Find cubic polynomial\\[\\begin{equation*}\n      f_{1}(x) = a_{1} + b_{1}x + c_{1}x^{2} + d_{1}x^{3}\n  \\end{equation*}\\]f(x)= f1(x) \\(x \\leq \\xi{}\\). Express \\(a_{1}\\),\\(b_{1}\\),\\(c_{1}\\),\\(d_{1}\\) terms \\(\\beta_{0}\\),\\(\\beta_{1}\\), \\(\\beta_{2}\\), \\(\\beta_{3}\\), \\(\\beta_{4}\\)En este caso, al que la función \\((x - \\xi{})^{3}_{+}\\), cada coeficiente corresponde su análogo ordenados de mayor menor, es decir \\(a_{1} = \\beta_{0}\\), \\(b_{1} = \\beta_{1}\\), \\(c_{1} = \\beta_{2}\\), \\(d_{1} = \\beta_{3}\\)(b) Find cubic polynomial\n\\[\\begin{equation*}\n      f_{2}(x) = a_{1} + b_{1}x + c_{1}x^{2} + d_{1}x^{3}\n  \\end{equation*}\\]\nf(x)= f2(x) \\(x > \\xi{}\\). Express \\(a_{2}\\),\\(b_{2}\\),\\(c_{2}\\),\\(d_{2}\\) terms \\(\\beta_{0}\\),\\(\\beta_{1}\\), \\(\\beta_{2}\\), \\(\\beta_{3}\\), \\(\\beta_{4}\\) now established f(x) piecewise polynomial.\nEn caso contrario la parte anterior la función partida se activa, primero debemos desarrollar el término \\((x-\\xi{})^{3}\\) asociado \\(\\beta_{4}\\)\n\\[\\begin{equation*}\n      \\beta_{4}(x-\\xi{})^{3} = \\left(x^{3} + -3x^{2} \\xi{} +3x\\xi{}^{2} - \\xi{}^{3}\\right)\\beta_{4} = \\beta_{4}x^{3} - 3\\beta_{4}x^{2} \\xi{} + 3\\beta_{4}x\\xi{}^{2} - \\beta_{4}\\xi{}^{3}\n  \\end{equation*}\\]\nPor lo tanto, remplazando en la especificación del modelo:\n\\[\\begin{equation*}\n      f(x) = \\beta_{0} + \\beta_{1}x + \\beta_{2}x^{2} + \\beta_{3}x^{3} + \\beta_{4}x^{3} - 3\\beta_{4}x^{2} \\xi{} + 3\\beta_{4}x\\xi{}^{2} - \\beta_{4}\\xi{}^{3}\n  \\end{equation*}\\]\nAgrupando términos,\n\\[\\begin{equation*}\n      f(x) = \\underbrace{(\\beta_{0} - \\beta_{4}\\xi{}^{3})}_{a_{2}} + \\overbrace{(\\beta_{1} + 3\\beta_{4}\\xi{}^{3})x}^{b_{2}} + \\underbrace{(\\beta_{2} - 3\\beta_{4}\\xi{})x^{2} (\\beta_{3}}_{c_{2}} + \\overbrace{\\beta_{4})x^{3}}^{d_{2}}\n  \\end{equation*}\\](b) Find cubic polynomial\\[\\begin{equation*}\n      f_{2}(x) = a_{1} + b_{1}x + c_{1}x^{2} + d_{1}x^{3}\n  \\end{equation*}\\]f(x)= f2(x) \\(x > \\xi{}\\). Express \\(a_{2}\\),\\(b_{2}\\),\\(c_{2}\\),\\(d_{2}\\) terms \\(\\beta_{0}\\),\\(\\beta_{1}\\), \\(\\beta_{2}\\), \\(\\beta_{3}\\), \\(\\beta_{4}\\) now established f(x) piecewise polynomial.En caso contrario la parte anterior la función partida se activa, primero debemos desarrollar el término \\((x-\\xi{})^{3}\\) asociado \\(\\beta_{4}\\)\\[\\begin{equation*}\n      \\beta_{4}(x-\\xi{})^{3} = \\left(x^{3} + -3x^{2} \\xi{} +3x\\xi{}^{2} - \\xi{}^{3}\\right)\\beta_{4} = \\beta_{4}x^{3} - 3\\beta_{4}x^{2} \\xi{} + 3\\beta_{4}x\\xi{}^{2} - \\beta_{4}\\xi{}^{3}\n  \\end{equation*}\\]Por lo tanto, remplazando en la especificación del modelo:\\[\\begin{equation*}\n      f(x) = \\beta_{0} + \\beta_{1}x + \\beta_{2}x^{2} + \\beta_{3}x^{3} + \\beta_{4}x^{3} - 3\\beta_{4}x^{2} \\xi{} + 3\\beta_{4}x\\xi{}^{2} - \\beta_{4}\\xi{}^{3}\n  \\end{equation*}\\]Agrupando términos,\\[\\begin{equation*}\n      f(x) = \\underbrace{(\\beta_{0} - \\beta_{4}\\xi{}^{3})}_{a_{2}} + \\overbrace{(\\beta_{1} + 3\\beta_{4}\\xi{}^{3})x}^{b_{2}} + \\underbrace{(\\beta_{2} - 3\\beta_{4}\\xi{})x^{2} (\\beta_{3}}_{c_{2}} + \\overbrace{\\beta_{4})x^{3}}^{d_{2}}\n  \\end{equation*}\\](c) Show \\(f_{1}(\\xi{})= f_{2}(\\xi{})\\). , \\(f(x)\\) continuous \\(\\xi{}\\)\nEvaluando ambas expresiones:\n\\[\\begin{equation*}\n      f_{1}(\\xi{}) = \\beta_{0} + \\beta_{1}\\xi{} + \\beta_{2}\\xi{}^{2} + \\beta_{3}\\xi{}^{3}\n  \\end{equation*}\\]\n\\[\\begin{equation*}\n      f_{2}(\\xi{}) = \\beta_{0} + \\beta_{1}\\xi{} + \\beta_{2}\\xi{}^{2} + \\beta_{3}\\xi{}^{3}\n  \\end{equation*}\\]\nPor lo tanto, podemos asumir que el valor funcional en el salto y límite tanto por izquierda y derecha coinciden por lo tanto, la función es continua.(c) Show \\(f_{1}(\\xi{})= f_{2}(\\xi{})\\). , \\(f(x)\\) continuous \\(\\xi{}\\)Evaluando ambas expresiones:\\[\\begin{equation*}\n      f_{1}(\\xi{}) = \\beta_{0} + \\beta_{1}\\xi{} + \\beta_{2}\\xi{}^{2} + \\beta_{3}\\xi{}^{3}\n  \\end{equation*}\\]\\[\\begin{equation*}\n      f_{2}(\\xi{}) = \\beta_{0} + \\beta_{1}\\xi{} + \\beta_{2}\\xi{}^{2} + \\beta_{3}\\xi{}^{3}\n  \\end{equation*}\\]Por lo tanto, podemos asumir que el valor funcional en el salto y límite tanto por izquierda y derecha coinciden por lo tanto, la función es continua.(d) Show \\(f^{''}_{1}(\\xi)=f^{''}_{2}(\\xi)\\). \\(f^{''}(x)\\) continuous \\(\\xi\\). Therefore, f(x) indeed cubic spline.\nTomando primeras derivadas de cada expresión:\n\\[\\begin{equation*}\n      f^{'}_{1} =  \\beta_{1} + 2\\beta_{2}x + 3\\beta_{3}x^{2}\n  \\end{equation*}\\]\n\\[\\begin{equation*}\n      f^{'}_{2} =  (\\beta_{1} + 3\\beta{4}\\xi{}^{2}) + 2(\\beta_{2} - 3\\beta_{4}\\xi)x + 3(\\beta_{3} + \\beta_{4})x^{2}\n  \\end{equation*}\\]\nDerivando nuevamente y evaluando:\n\\[\\begin{equation*}\n      f^{''}_{1} =  2\\beta_{2} + 6\\beta_{3}x \\longrightarrow f^{''}_{1}(\\xi) = 2\\beta_{2} + 6\\beta_{3}\\xi\n  \\end{equation*}\\]\n\\[\\begin{equation*}\n      f^{''}_{2} =  2(\\beta_{2}-\\beta_{4})\\xi + 6(\\beta_{3} + \\beta_{4})x \\longrightarrow f^{''}_{2}(\\xi{}) = 2\\beta_{2} + 6\\beta_{3}\\xi\n  \\end{equation*}\\](d) Show \\(f^{''}_{1}(\\xi)=f^{''}_{2}(\\xi)\\). \\(f^{''}(x)\\) continuous \\(\\xi\\). Therefore, f(x) indeed cubic spline.Tomando primeras derivadas de cada expresión:\\[\\begin{equation*}\n      f^{'}_{1} =  \\beta_{1} + 2\\beta_{2}x + 3\\beta_{3}x^{2}\n  \\end{equation*}\\]\\[\\begin{equation*}\n      f^{'}_{2} =  (\\beta_{1} + 3\\beta{4}\\xi{}^{2}) + 2(\\beta_{2} - 3\\beta_{4}\\xi)x + 3(\\beta_{3} + \\beta_{4})x^{2}\n  \\end{equation*}\\]Derivando nuevamente y evaluando:\\[\\begin{equation*}\n      f^{''}_{1} =  2\\beta_{2} + 6\\beta_{3}x \\longrightarrow f^{''}_{1}(\\xi) = 2\\beta_{2} + 6\\beta_{3}\\xi\n  \\end{equation*}\\]\\[\\begin{equation*}\n      f^{''}_{2} =  2(\\beta_{2}-\\beta_{4})\\xi + 6(\\beta_{3} + \\beta_{4})x \\longrightarrow f^{''}_{2}(\\xi{}) = 2\\beta_{2} + 6\\beta_{3}\\xi\n  \\end{equation*}\\]","code":""},{"path":"moving-beyond-linearity.html","id":"ejercicio-2","chapter":"Capítulo 4 Moving Beyond Linearity","heading":"Ejercicio 2","text":"Suppose curve \\(\\hat{g}\\) computed smoothly fit set n points using following formula:\\[\\begin{equation}\n    \\hat{g} = arg ~ min _{g} \\left(\\sum_{}^{n}{\\left(y_{} - g(x_{})\\right)^{2}} + \\lambda \\int{\\left[g^{m}(x)\\right]^{2}dx} \\right)\n\\end{equation}\\]() \\(\\lambda = \\infty\\), \\(m=0\\)\nEn este caso, la expresión anterior queda determinada de la siguiente manera:\n\\[\\begin{equation}\n      \\hat{g} = arg ~ min _{g} \\left(\\sum_{}^{n}{\\left(y_{} - g(x_{})\\right)^{2}} + \\lambda \\int{\\left[g^(x)\\right]^{2}dx} \\right)\n  \\end{equation}\\]\nComo la expresión debe ser finita, al que estamos frente un valor extremo del parámetro de penalización la integral debe ser cero en todo su recorrido, por lo tanto \\(g(x) = 0\\).() \\(\\lambda = \\infty\\), \\(m=0\\)En este caso, la expresión anterior queda determinada de la siguiente manera:\\[\\begin{equation}\n      \\hat{g} = arg ~ min _{g} \\left(\\sum_{}^{n}{\\left(y_{} - g(x_{})\\right)^{2}} + \\lambda \\int{\\left[g^(x)\\right]^{2}dx} \\right)\n  \\end{equation}\\]Como la expresión debe ser finita, al que estamos frente un valor extremo del parámetro de penalización la integral debe ser cero en todo su recorrido, por lo tanto \\(g(x) = 0\\).(b) \\(\\lambda = \\infty\\), \\(m=1\\)\nNuevamente, al estar en el límite de \\(\\lambda\\), revisemos la expresión\n\\[\\begin{equation}\n      \\hat{g} = arg ~ min _{g} \\left(\\sum_{}^{n}{\\left(y_{} - g(x_{})\\right)^{2}} + \\lambda \\int{\\left[g^(x)^{1}\\right]^{2}dx} \\right)\n  \\end{equation}\\]\nDe forma análoga al se pide anterior, la derivada \\(g^{'}(x) = 0\\), entonces \\(g(x) = c\\).(b) \\(\\lambda = \\infty\\), \\(m=1\\)Nuevamente, al estar en el límite de \\(\\lambda\\), revisemos la expresión\\[\\begin{equation}\n      \\hat{g} = arg ~ min _{g} \\left(\\sum_{}^{n}{\\left(y_{} - g(x_{})\\right)^{2}} + \\lambda \\int{\\left[g^(x)^{1}\\right]^{2}dx} \\right)\n  \\end{equation}\\]De forma análoga al se pide anterior, la derivada \\(g^{'}(x) = 0\\), entonces \\(g(x) = c\\).(c) \\(\\lambda = \\infty\\), \\(m=2\\)\n\\[\\begin{equation}\n      \\hat{g} = arg ~ min _{g} \\left(\\sum_{}^{n}{\\left(y_{} - g(x_{})\\right)^{2}} + \\lambda \\int{\\left[g^(x)^{2}\\right]^{2}dx} \\right)\n  \\end{equation}\\]\nDe forma análoga al se pide anterior, la derivada \\(g^{''}(x) = 0\\), entonces \\(g(x) = ax+b\\).(c) \\(\\lambda = \\infty\\), \\(m=2\\)\\[\\begin{equation}\n      \\hat{g} = arg ~ min _{g} \\left(\\sum_{}^{n}{\\left(y_{} - g(x_{})\\right)^{2}} + \\lambda \\int{\\left[g^(x)^{2}\\right]^{2}dx} \\right)\n  \\end{equation}\\]De forma análoga al se pide anterior, la derivada \\(g^{''}(x) = 0\\), entonces \\(g(x) = ax+b\\).(d) \\(\\lambda = \\infty\\), \\(m=3\\)\n\\[\\begin{equation}\n      \\hat{g} = arg ~ min _{g} \\left(\\sum_{}^{n}{\\left(y_{} - g(x_{})\\right)^{2}} + \\lambda \\int{\\left[g^(x)^{3}\\right]^{2}dx} \\right)\n  \\end{equation}\\]\nDe forma análoga al se pide anterior, la derivada \\(g^{'''}(x) = 0\\), entonces \\(g(x) = ax^{2} + bx +c\\).(d) \\(\\lambda = \\infty\\), \\(m=3\\)\\[\\begin{equation}\n      \\hat{g} = arg ~ min _{g} \\left(\\sum_{}^{n}{\\left(y_{} - g(x_{})\\right)^{2}} + \\lambda \\int{\\left[g^(x)^{3}\\right]^{2}dx} \\right)\n  \\end{equation}\\]De forma análoga al se pide anterior, la derivada \\(g^{'''}(x) = 0\\), entonces \\(g(x) = ax^{2} + bx +c\\).(e) \\(\\lambda = 0\\), \\(m=3\\)\nEn este caso, estamos frente un spline cúbico sin suavizado y la estimación es por MCO.(e) \\(\\lambda = 0\\), \\(m=3\\)En este caso, estamos frente un spline cúbico sin suavizado y la estimación es por MCO.","code":""},{"path":"moving-beyond-linearity.html","id":"ejercicio-3","chapter":"Capítulo 4 Moving Beyond Linearity","heading":"Ejercicio 3","text":"Suppose fit curve basis functions \\(b_{1}(X) = X\\), \\(b_{2} = (X-1)^{2}(X \\leq 1)\\). fit linear regression model\\[\\begin{equation*}\n    Y = \\beta_{0} + \\beta_{1}b_{1}(X) + \\beta_{2}b_{2}(X) + \\varepsilon\n\\end{equation*}\\]obtain coefficient estiamtes \\(\\hat{\\beta_{0}} = 1\\), \\(\\hat{\\beta_{1}} = 1\\), \\(\\hat{\\beta_{2}} = -2\\). Sketch estimated curve \\(X = −2\\) \\(X = 2\\). Note intercepts, slopes, relevant informationremplazando los coeficentes estimados, obtenemos la siguiente función de regresión:\\[\\begin{equation*}\n    \\hat{f}(X) = 1 + X - 2(X-1)^{2}(X \\leq 1)\n\\end{equation*}\\]Es claro que la izquierda de \\(1\\) la función es una recta, por tanto su pendiente es uno. la derecha,\nla derivada es igual \\(f'(x) = -4x + 5\\). La ordenada en el origen para la función de la izquierda es 1, mientras\nque para la de la derecha es -1. En 1 la función es continua y derivable. La función obtiene su máximo en \\(X = 5/4\\)","code":"\n\nf <- function(x) {\n    1 + x + -2 * (x - 1)^2 * (x >= 1)\n}\n\nx <- seq(-2,2,0.01)\n\ndf <- data.frame(x,y =f(x))\n\n\nlibrary(ggplot2)\n\nggplot(\n    df,\n    aes(x = x, y = y)\n) +\ngeom_line(\n    colour = \"blue\",\n    size = 1.2\n) +\ngeom_vline(\n    xintercept = 1,\n    colour = \"green\",\n    size = 1.2,\n    linetype = \"dashed\"\n) +\ngeom_vline(xintercept = 0) +\ngeom_hline(yintercept = 0) +\nannotate(\n    \"text\",\n    x = -1,\n    y = 1,\n    label = \"f(x) = 1 +  x\",\n    parse = FALSE\n) + \nannotate(\n    \"text\",\n    x = 1.5,\n    y = 1,\n    label = \"f(x) == -2*x^{2} +5 *x -1\",\n    parse = TRUE\n) +\ntheme(\n    aspect.ratio = 1\n)"},{"path":"moving-beyond-linearity.html","id":"ejercicio-4-1","chapter":"Capítulo 4 Moving Beyond Linearity","heading":"Ejercicio 4","text":"Suppose fit curve basis functions \\(b_{1}(X) = (0 \\leq X \\leq 2) - (X-1)(1 \\leq X \\leq 2)\\), \\(b_{2} = (X-3)(3 \\leq X \\leq 4) + (4 < X \\leq 5)\\). fit linear regression model\\[\\begin{equation*}\n    Y = \\beta_{0} + \\beta_{1}b_{1}(X) + \\beta_{2}b_{2}(X) + \\varepsilon\n\\end{equation*}\\]obtain coefficient estiamtes \\(\\hat{\\beta_{0}} = 1\\), \\(\\hat{\\beta_{1}} = 1\\), \\(\\hat{\\beta_{2}} = 3\\). Sketch estimated curve \\(X = −2\\) \\(X = 2\\). Note intercepts, slopes, relevant informationLa función de regresión puede escribirse como:\\[\\begin{equation}\n    \\hat{f}(X) = 1 + (0 \\leq X \\leq 2) - (X-1)(1 \\leq X \\leq 2) + 3 \\times (X-3)(3 \\leq X \\leq 4) + (4 < X \\leq 5)\n\\end{equation}\\]la izquierda de cero la pendiente vale cero, entre 0 y 1, la función es contante y vale 2, mientras que luego de 1 es una recta con pendiente unitaria negativa.","code":"\n\nf <- function(x) {\n    1 + (x >= 0 & x <= 2) - (x-1)*(x >= 1 & x <= 2) + 3 * (x-3)*(x >= 3 & x <=4) + x*(x > 4 & x <= 5)\n}\n\nx <- seq(-2,2,0.01)\n\ndf <- data.frame(x,y =f(x))\n\n\nlibrary(ggplot2)\n\nggplot(\n    df,\n    aes(x = x, y = y)\n) +\ngeom_line(\n    colour = \"blue\",\n    size = 1.2\n) +\ngeom_vline(\n    xintercept = 1,\n    colour = \"green\",\n    size = 1.2,\n    linetype = \"dashed\"\n) +\ngeom_vline(\n    xintercept = 2,\n    colour = \"green\",\n    size = 1.2,\n    linetype = \"dashed\"\n) + \ngeom_vline(\n    xintercept = 0,\n    colour = \"green\",\n    size = 1.2,\n    linetype = \"dashed\"\n) +\ngeom_vline(xintercept = 0) +\ngeom_hline(yintercept = 0) +\ntheme(\n    aspect.ratio = 1\n)"},{"path":"moving-beyond-linearity.html","id":"ejercicio-5-1","chapter":"Capítulo 4 Moving Beyond Linearity","heading":"Ejercicio 5","text":"Consider tow curves \\(\\hat{g}_{1}\\) \\(\\hat{g}_{2}\\), defined \\[\\begin{equation}\n    \\hat{g}_{1} = arg ~ min _{g} \\left(\\sum_{}^{n}{\\left(y_{} - g(x_{})\\right)^{2}} + \\lambda \\int{\\left[g^{3}(x)\\right]^{2}dx} \\right)\n\\end{equation}\\]\\[\\begin{equation}\n    \\hat{g}_{2} = arg ~ min _{g} \\left(\\sum_{}^{n}{\\left(y_{} - g(x_{})\\right)^{2}} + \\lambda \\int{\\left[g^{4}(x)\\right]^{2}dx} \\right)\n\\end{equation}\\]() \\(\\lambda{} \\rightarrow \\infty\\), \\(\\hat{g}_{1}\\) \\(\\hat{g}_{2}\\) smaller training RSS?\nLas curvas resultantes resultan ser de órden cuadratíco y cúbico respectivamente, la segunda curva al tener un órden polinimico mayor es un modelo\nmas flexible, por tanto, tiene un menor error de entrenamiento.() \\(\\lambda{} \\rightarrow \\infty\\), \\(\\hat{g}_{1}\\) \\(\\hat{g}_{2}\\) smaller training RSS?Las curvas resultantes resultan ser de órden cuadratíco y cúbico respectivamente, la segunda curva al tener un órden polinimico mayor es un modelo\nmas flexible, por tanto, tiene un menor error de entrenamiento.(b) \\(\\lambda{} \\rightarrow \\infty\\), \\(\\hat{g}_{1}\\) \\(\\hat{g}_{2}\\) smaller test RSS?\nDepende de como sea la relación funcional y de los datos, en base esto podemos afirmar quien tiene un mayor RSS fuera de la muestra de entrenamiento.(b) \\(\\lambda{} \\rightarrow \\infty\\), \\(\\hat{g}_{1}\\) \\(\\hat{g}_{2}\\) smaller test RSS?Depende de como sea la relación funcional y de los datos, en base esto podemos afirmar quien tiene un mayor RSS fuera de la muestra de entrenamiento.(c) \\(\\lambda = 0\\), \\(\\hat{g}_{1}\\) \\(\\hat{g}_{2}\\) smaller training test RSS?\nEn este caso la restricción de suavizado tiene efecto, por lo tanto \\(\\hat{g}_{1} = \\hat{g_{2}}\\), en consecuencia tienen el mismo RSS tanto en la muestra de entrenamiento y testing.(c) \\(\\lambda = 0\\), \\(\\hat{g}_{1}\\) \\(\\hat{g}_{2}\\) smaller training test RSS?En este caso la restricción de suavizado tiene efecto, por lo tanto \\(\\hat{g}_{1} = \\hat{g_{2}}\\), en consecuencia tienen el mismo RSS tanto en la muestra de entrenamiento y testing.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
